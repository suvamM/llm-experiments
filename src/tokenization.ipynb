{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/the-verdict.txt', <http.client.HTTPMessage at 0x21eb62954d0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through th\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters in the text:\", len(raw_text))\n",
    "print(raw_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '', 'world!', 'this', 'is', 'a', 'test', '']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text using regex\n",
    "import re\n",
    "text = \"Hello, world! this is a test.\"\n",
    "\n",
    "result = re.split(r\"[,.]|\\s\", text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 71, 73, 31298, 31298, 18435, 11, 995, 0, 428, 318, 257, 1332, 13, 198, 220, 50256, 1212, 318, 257, 649, 1627, 13, 3914, 338, 766, 703, 428, 11241, 7509, 2499, 13]\n",
      "jhjhdhd Hello, world! this is a test.\n",
      " <|endoftext|>This is a new line. Let's see how this tokenizer works.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"jhjhdhd Hello, world! this is a test.\\n <|endoftext|>\"\n",
    "    \"This is a new line. Let's see how this tokenizer works.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "\n",
    "print(tokenizer.decode(integers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last,\n",
    "                            num_workers=num_workers)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(txt, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "        [ 0.3486,  0.6603, -0.2196, -0.3792],\n",
      "        [-0.1606, -0.4015,  0.6957, -1.8061],\n",
      "        [ 1.8960, -0.1750,  1.3689, -1.6033],\n",
      "        [-0.7849, -1.4096, -0.4076,  0.7953],\n",
      "        [ 0.9985,  0.2212,  1.8319, -0.3378]], device='xpu:0',\n",
      "       requires_grad=True)\n",
      "tensor([[[ 0.3486,  0.6603, -0.2196, -0.3792],\n",
      "         [-0.1606, -0.4015,  0.6957, -1.8061],\n",
      "         [ 1.8960, -0.1750,  1.3689, -1.6033],\n",
      "         [-0.7849, -1.4096, -0.4076,  0.7953]]], device='xpu:0',\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Trying out a small token embedding model\n",
    "\n",
    "input_ids = torch.tensor([[1, 2, 3, 4]]).to('xpu')\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 4\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim).to('xpu')\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "embeddings = embedding_layer(input_ids)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the full embedding model\n",
    "vocab_size = 50257 # size of the gpt-2 vocabulary\n",
    "output_dim = 256\n",
    "token_embeding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(txt, batch_size=8, max_length=max_length, stride=max_length, shuffle=False, num_workers=8)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
